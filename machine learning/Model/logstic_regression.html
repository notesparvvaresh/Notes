<!DOCTYPE html>
<html>
<head>
  <title>Logistic Regression</title>
</head>
<body>
  <h2>Mathematical Background of Logistic Regression</h2>
  <ol>
    <li>
      <h3>Model Representation</h3>
      <p>Logistic regression models the probability P(Y=1 | X = x) as:</p>
      <p>P(Y=1 | X = x) = σ(w<sup>T</sup>x)</p>
      <p>where:</p>
      <ul>
        <li>X = [X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>n</sub>] is the vector of predictor variables.</li>
        <li>w = [w<sub>0</sub>, w<sub>1</sub>, ..., w<sub>n</sub>] is the vector of weights (parameters) including the intercept w<sub>0</sub>.</li>
        <li>σ(z) is the logistic (sigmoid) function defined as:</li>
      </ul>
      <p>σ(z) = 1 / (1 + e<sup>-z</sup>)</p>
    </li>
    <li>
      <h3>Logistic (Sigmoid) Function</h3>
      <p>The sigmoid function maps any real-valued number into the range [0, 1]. For logistic regression, it is defined as:</p>
      <p>σ(z) = 1 / (1 + e<sup>-z</sup>)</p>
      <p>This function is S-shaped and asymptotically approaches 0 as z approaches -∞ and 1 as z approaches +∞.</p>
    </li>
    <li>
      <h3>Decision Boundary</h3>
      <p>The decision rule for classifying an observation is based on a threshold (usually 0.5). If the predicted probability is greater than or equal to 0.5, the observation is classified as 1; otherwise, it is classified as 0.</p>
    </li>
    <li>
      <h3>Cost Function (Log-Loss)</h3>
      <p>The cost function used in logistic regression is the log-likelihood function, which measures the fit of the model. The cost function (or loss function) is:</p>
      <p>J(w) = -1 / m * Σ [y<sup>(i)</sup> * log(y_hat<sup>(i)</sup>) + (1 - y<sup>(i)</sup>) * log(1 - y_hat<sup>(i)</sup>)]</p>
      <p>where:</p>
      <ul>
        <li>m is the number of training examples.</li>
        <li>y<sup>(i)</sup> is the actual label of the i-th training example.</li>
        <li>y_hat<sup>(i)</sup> = σ(w<sup>T</sup>x<sup>(i)</sup>) is the predicted probability for the i-th training example.</li>
      </ul>
    </li>
    <li>
      <h3>Gradient Descent for Optimization</h3>
      <p>To find the optimal weights w that minimize the cost function, we use gradient descent. The gradient of the cost function with respect to w is:</p>
      <p>∇J(w) = 1 / m * Σ (y_hat<sup>(i)</sup> - y<sup>(i)</sup>) * x<sup>(i)</sup></p>
      <p>The weights are updated iteratively using the gradient descent update rule:</p>
      <p>w := w - α * ∇J(w)</p>
      <p>where α is the learning rate.</p>
    </li>
    <li>
      <h3>Interpretation of Coefficients</h3>
      <p>The coefficients w in logistic regression represent the log-odds of the outcome variable. Specifically, the coefficient w<sub>j</sub> associated with predictor X<sub>j</sub> can be interpreted as the change in the log-odds of the outcome for a one-unit increase in X<sub>j</sub>, holding all other predictors constant.</p>
    </li>
  </ol>

  <h2>Logistic Regression Algorithm</h2>
  <ol>
    <li>Initialization: Initialize the weights w with small random values or zeros.</li>
    <li>Compute the Prediction: Compute the predicted probability y_hat for each training example using the logistic function.</li>
    <li>Compute the Cost: Calculate the cost function J(w).</li>
    <li>Compute the Gradient: CalculateApologies for the update weights</li>
    <li> repeat step 2 - 4


