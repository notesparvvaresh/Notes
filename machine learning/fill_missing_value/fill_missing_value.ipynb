{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed6f054-9489-4ab9-8ad9-6a493bd6a216",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "Missing values are a common problem in machine learning and data science projects. Failure to handle missing data correctly can skew the results of machine learning models or reduce the model's accuracy. To overcome these obstacles, we must either carefully remove missing values or carefully complete them. Either way, our goal is to create a complete dataset that will put our analysis on a solid footing. Ignoring missing data can directly affect the performance and reliability of our models. Therefore, it is extremely important to know the different techniques for dealing with missing values and choose the right method. While dealing with missing values is not our ultimate goal, it plays a critical role in data preprocessing steps and can help to significantly improve model performance. In this article, I will explain the causes and types of missing data and the advantages and disadvantages of various methods to deal with this problem.\n",
    "\n",
    "Missing data can be caused by merging data sources, accidents or measurement errors during data collection, lack of records, respondent unwillingness to provide information, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee3963-d8bd-4698-be47-95eaf4345515",
   "metadata": {},
   "source": [
    "# 2. Dataset\n",
    "In this article, I will use the “UCI Heart Disease Data” set available in the UCI Data Repository. This dataset aims to predict whether a patient has heart disease based on certain characteristics. You can access the dataset here;\n",
    "\n",
    "https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data\n",
    "https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "In the dataset, the target variable is given in 5 groups. These groups will be reduced to 2 (with heart disease, and without heart disease) and evaluated.\n",
    "\n",
    "In addition, since this study does not focus on model development, I will encode the categorical variables and make the entire dataset numerical. The dataset I will use in the scope of the article looks as follows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b656f5-7fe3-4503-9bbf-3620af0b4960",
   "metadata": {},
   "source": [
    "# import library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a00608d-4f04-41d5-8f5c-dd20185aca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For linear algebra operations and numerical computations\n",
    "import pandas as pd # For data analysis and manipulation using data frames\n",
    "\n",
    "import missingno as msno # For visualizing missing data patterns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder # For converting categorical data to numerical values\n",
    "from sklearn.model_selection import train_test_split # For splitting data into training and testing sets\n",
    "from sklearn.metrics import accuracy_score # For calculating the accuracy score of a classification model\n",
    "from sklearn.impute import KNNImputer # For imputing missing values using K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors classification model\n",
    "from sklearn.linear_model import LinearRegression # Linear regression model\n",
    "from sklearn.ensemble import RandomForestRegressor # Random Forest regression model\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest classifier model\n",
    "from sklearn.metrics import mean_squared_error # Performance metric for regression models\n",
    "from sklearn.impute import SimpleImputer # For imputing missing values with a simple strategy\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier # XGBoost classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13e244-420a-417f-a987-e45ab35a7950",
   "metadata": {},
   "source": [
    "# open and see dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1da3695-5f20-4cc6-ad7a-f7ff1465ab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalch</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>typical angina</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>True</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>150.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.3</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>108.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.5</td>\n",
       "      <td>flat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>129.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.6</td>\n",
       "      <td>flat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable defect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "      <td>187.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.5</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>atypical angina</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>172.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.4</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age     sex    dataset               cp  trestbps   chol    fbs  \\\n",
       "0   1   63    Male  Cleveland   typical angina     145.0  233.0   True   \n",
       "1   2   67    Male  Cleveland     asymptomatic     160.0  286.0  False   \n",
       "2   3   67    Male  Cleveland     asymptomatic     120.0  229.0  False   \n",
       "3   4   37    Male  Cleveland      non-anginal     130.0  250.0  False   \n",
       "4   5   41  Female  Cleveland  atypical angina     130.0  204.0  False   \n",
       "\n",
       "          restecg  thalch  exang  oldpeak        slope   ca  \\\n",
       "0  lv hypertrophy   150.0  False      2.3  downsloping  0.0   \n",
       "1  lv hypertrophy   108.0   True      1.5         flat  3.0   \n",
       "2  lv hypertrophy   129.0   True      2.6         flat  2.0   \n",
       "3          normal   187.0  False      3.5  downsloping  0.0   \n",
       "4  lv hypertrophy   172.0  False      1.4    upsloping  0.0   \n",
       "\n",
       "                thal  num  \n",
       "0       fixed defect    0  \n",
       "1             normal    2  \n",
       "2  reversable defect    1  \n",
       "3             normal    0  \n",
       "4             normal    0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/reza/Desktop/git hub/Notes/machine learning/fill_missing_value/heart_disease_uci.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f8614f-f8fe-4f58-be4b-306ea4df066c",
   "metadata": {},
   "source": [
    "# report of null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c3719b-b3a9-40d8-b610-d034e43a254f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "age           0\n",
       "sex           0\n",
       "dataset       0\n",
       "cp            0\n",
       "trestbps     59\n",
       "chol         30\n",
       "fbs          90\n",
       "restecg       2\n",
       "thalch       55\n",
       "exang        55\n",
       "oldpeak      62\n",
       "slope       309\n",
       "ca          611\n",
       "thal        486\n",
       "num           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ff772-0057-4392-a624-5f8b932f9499",
   "metadata": {},
   "source": [
    "# clean data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282b0fa0-b251-4965-a5f2-1a0284370e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"num\"] = df[\"num\"].replace({2 : 1, 3 : 1, 4 : 1})\n",
    "    df = df.drop(\"id\" , axis=1)\n",
    "    categorical_columns = df.select_dtypes(['object']).columns\n",
    "    LabelEncoder_model = LabelEncoder()\n",
    "    \n",
    "    for column in categorical_columns:\n",
    "        df[column] = LabelEncoder_model.fit_transform(df[column])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c1373-b9e0-4be5-b119-ef5a247b70e8",
   "metadata": {},
   "source": [
    "# spilt dataset to test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b04a4e-ac4f-4587-9af6-9d77572b896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    X = df.drop(\"num\", axis=1)\n",
    "    y = df[\"num\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d783c8e0-e72e-46b5-8a4d-519de7ad48fa",
   "metadata": {},
   "source": [
    "# train a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a668d7-e40a-4362-8ede-73c9c5b9497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_rf(X_train, X_test, y_train, y_test, method) -> dict:\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return {\n",
    "        \"model\" : model,\n",
    "        \"y_pred\" : y_pred,\n",
    "        \"accuracy\" : accuracy,\n",
    "        \"method\" : method\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa3fb9-5bb6-4d26-9399-e9c48da824ef",
   "metadata": {},
   "source": [
    "# 1- Delete Missing Data\n",
    "\n",
    "If models that do not tolerate missing data are to be used, the missing values must be removed from the data set. The simplest approach is to delete all samples that contain missing values. Missing values can be eliminated by removing variables that contain at least one missing value. Similarly, removing all columns with missing values from the data set is another approach. Before using these approaches, however, some aspects of the data need to be carefully considered. The size of the data after deletion of missing data, the randomness of missing data, or the ratio of missing data to the entire data set are critical points to consider when applying these approaches.\n",
    "\n",
    "\n",
    "This category consists of 3 stages. These stages are:\n",
    "\n",
    "* Pairwise Deletions\n",
    "* Listwise Deletions\n",
    "* Dropping Entire Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee21712-6192-458f-a6be-eb237aee87c2",
   "metadata": {},
   "source": [
    "## 1.1 Pairwise Deletions\n",
    "\n",
    "If each data point has at least one missing value, that row of data is deleted from the entire list. \n",
    "This method can be used when the amount of missing data is small and the missing values are randomly distributed.\n",
    "\n",
    "\n",
    "- Pros:\n",
    "\n",
    "It is very useful for simplicity and computational efficiency.\n",
    "\n",
    "\n",
    "- Cons:\n",
    "\n",
    "It leads to a loss of information and a reduction in the number of samples. This can lead to insufficient data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56948604-4f93-49c8-814c-fa79a175d76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  RandomForestClassifier()  |  accuracy :  0.8387096774193549  | method :  pairwise \n"
     ]
    }
   ],
   "source": [
    "df = clean_data(df)\n",
    "\n",
    "df_pairwise = df.dropna()\n",
    "X_train, X_test, y_train, y_test = split_data(df_pairwise)\n",
    "result_pairwise = train_model_rf(X_train, X_test, y_train, y_test, \"pairwise \")\n",
    "\n",
    "print(\"model : \", result_pairwise[\"model\"], \" | \" , \"accuracy : \",  result_pairwise[\"accuracy\"], \" | method : \", result_pairwise[\"method\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f41959-8442-4de0-b336-8b5a90286215",
   "metadata": {},
   "source": [
    "## 1.2 Listwise Deletions\n",
    "\n",
    "All data instances with missing data from more than one variable are deleted. This method can be used when the amount of missing data is large or when the missing values are distributed according to a certain pattern.\n",
    "\n",
    "\n",
    "- Pros:\n",
    "\n",
    "As with Pairwise Deletions, it is very simple and low-cost to implement.\n",
    "- Cons:\n",
    "\n",
    "It may reduce the number of samples. This can lead to a destabilization of the dataset and loss of correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b63f3e-a7ad-4f18-9ad3-6f0639903253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  RandomForestClassifier()  |  accuracy :  0.8870967741935484  | method :  listwise\n"
     ]
    }
   ],
   "source": [
    "df_listwise = df.dropna(subset=['trestbps', 'chol', 'thalch', 'oldpeak', 'ca'])\n",
    "X_train, X_test, y_train, y_test = split_data(df_listwise)\n",
    "\n",
    "result_listwise = train_model_rf(X_train, X_test, y_train, y_test, \"listwise\")\n",
    "\n",
    "\n",
    "print(\"model : \", result_listwise[\"model\"], \" | \" , \"accuracy : \",  result_listwise[\"accuracy\"], \" | method : \", result_listwise[\"method\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45f06a-fd2a-4a0c-b456-37ab8d69770f",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3752f-08fc-48ba-be7a-bfa0ec658237",
   "metadata": {},
   "source": [
    "##  1.3 Dropping Entire Columns\n",
    "\n",
    "All columns with too much missing data are deleted from the dataset. \n",
    "This method can be used when the amount of missing data is very large and the columns with missing data are not important for the analysis. However, this approach should only be used when there are many empty values in the column.\n",
    "\n",
    "- Pros:\n",
    "\n",
    "Like other deletions, this method offers a simple and computationally cost-effective solution.\n",
    "- Cons:\n",
    "\n",
    "Deleting all columns with missing values may result in the loss of important information in the dataset. This can negatively affect the accuracy and reliability of the model.\n",
    "While the simplicity of this method may be attractive, deleting columns has serious consequences for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b9415cf-1d11-48d3-a035-9b7d2c841500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  RandomForestClassifier()  |  accuracy :  0.8315217391304348  | method :  dropping entire columns\n"
     ]
    }
   ],
   "source": [
    "df_dropping_entire_columns = df.dropna(axis=1)\n",
    "X_train, X_test, y_train, y_test = split_data(df_dropping_entire_columns)\n",
    "result_dropping_entire_columns = train_model_rf(X_train, X_test, y_train, y_test, \"dropping entire columns\")\n",
    "\n",
    "print(\"model : \", result_dropping_entire_columns[\"model\"], \" | \" , \"accuracy : \",  result_dropping_entire_columns[\"accuracy\"], \" | method : \", result_dropping_entire_columns[\"method\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcee65f-3972-4e54-8b96-aa3d475a9882",
   "metadata": {},
   "source": [
    "# 2. Statistically Filling Missing Data (Mean-Median-Constant)\n",
    "Another approach to handling missing data is to statistically fill them in. The filling process uses relationships between non-missing data.\n",
    "\n",
    "Some methods of this approach include filling in missing data with mean, median, or constant if the data are numeric and mode if the data are categorical.\n",
    "\n",
    "* . Mean and Median\n",
    "\n",
    "Mean: This is the most common method of filling in missing values in numeric columns. This method may not be appropriate if there are outliers in the dataset. In such cases, outliers need to be addressed first.\n",
    "\n",
    "Median: This is the value that divides the data series in half when we sort the dataset from smallest to largest. It is better to use the median if there are outliers in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529186b-3241-4eaf-be1b-ac59ba648103",
   "metadata": {},
   "source": [
    "def statistic(X_train : pd.DataFrame , \n",
    "              X_test : pd.DataFrame,\n",
    "             method : str) -> list:\n",
    "    imputer = SimpleImputer(strategy=method)\n",
    "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    return X_train_imputed, X_test_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf6330-e498-4882-bdff-5ee06102572c",
   "metadata": {},
   "source": [
    "# 2.1 mean\n",
    "\n",
    "Mean: This is the most common method of filling in missing values in numeric columns. This method may not be appropriate if there are outliers in the dataset. In such cases, outliers need to be addressed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c0301c2-64f4-4ed8-97dc-5b80e305dec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  RandomForestClassifier()  |  accuracy :  0.8315217391304348  | method :  mean\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(df)\n",
    "X_train_imputed, X_test_imputed = statistic(X_train , X_test , \"mean\")\n",
    "\n",
    "result_mean = train_model_rf(X_train, X_test, y_train, y_test, \"mean\")\n",
    "\n",
    "print(\"model : \", result_mean[\"model\"], \" | \" , \"accuracy : \",  result_mean[\"accuracy\"], \" | method : \", result_mean[\"method\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf57e8-c898-48dd-89d3-1c785e43410a",
   "metadata": {},
   "source": [
    "# 2.1 median\n",
    "\n",
    "Median: This is the value that divides the data series in half when we sort the dataset from smallest to largest. \n",
    "It is better to use the median if there are outliers in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5eabbea1-dcf0-4938-8718-565d78bfae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  RandomForestClassifier()  |  accuracy :  0.842391304347826  | method :  median\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(df)\n",
    "X_train_imputed, X_test_imputed = statistic(X_train , X_test , \"median\")\n",
    "\n",
    "result_median = train_model_rf(X_train, X_test, y_train, y_test, \"median\")\n",
    "\n",
    "print(\"model : \", result_median[\"model\"], \" | \" , \"accuracy : \",  result_median[\"accuracy\"], \" | method : \", result_median[\"method\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926a5d3-f787-42f0-9cc4-515b5fd0e22d",
   "metadata": {},
   "source": [
    "###  Pros:\n",
    "\n",
    "* Easy to implement and fast.\n",
    "* Minimizes information loss.\n",
    "* Preserves the overall performance of the model.\n",
    "* Conserves the size of the data set.\n",
    "* More sensitive to outliers (Median Imputation).\n",
    "### Cons:\n",
    "\n",
    "* May increase the impact of outliers (Mean Imputation).\n",
    "* It can change the distribution between values.\n",
    "* Distort the original structure of missing values.\n",
    "* Reduces the natural variance of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f0024-75f8-46ec-8bb6-30a0e659b0a9",
   "metadata": {},
   "source": [
    "## 2.3 Constant\n",
    "\n",
    "Constant filling is the process of filling missing values with a constant value (0, 100, 999, etc.). \n",
    "Although it has its advantages, it also has many disadvantages.\n",
    "\n",
    "\n",
    "- Pros:\n",
    "\n",
    "Like other filling methods, it is a simple and quick approach.\n",
    "- Cons:\n",
    "\n",
    "It may cause a loss of information in the dataset as it fills missing values with a random or meaningless value.\n",
    "If an appropriate constant value is not chosen for missing values, it may negatively affect the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa980758-b163-4a36-ae49-32e8b3a2674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Constant(X_train : pd.DataFrame , \n",
    "             X_test : pd.DataFrame,\n",
    "             method : str,\n",
    "             value : int) -> list:\n",
    "    imputer = SimpleImputer(strategy=method, fill_value=value)\n",
    "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    return X_train_imputed, X_test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d0d284f-bf4c-4115-8b60-2c51e8683315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  RandomForestClassifier()  |  accuracy :  0.842391304347826  | method :  median\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(df)\n",
    "X_train_imputed, X_test_imputed = Constant(X_train , X_test , \"constant\", 999)\n",
    "\n",
    "result_constant = train_model_rf(X_train, X_test, y_train, y_test, \"constant\")\n",
    "\n",
    "print(\"model : \", result_median[\"model\"], \" | \" , \"accuracy : \",  result_median[\"accuracy\"], \" | method : \", result_median[\"method\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108cde87-731b-4e1d-8bd0-44138ff471d3",
   "metadata": {},
   "source": [
    "# 4. Advanced Imputation Techniques (Prediction of Missing Values) (KNN — Linear Regression — Random Forest Regressor)\n",
    "In previous methods for handling missing values, we did not take advantage of the correlation between the variable containing the missing value and other variables. Using other features that do not have missing values can be used to estimate missing values. Depending on the nature of the feature with missing values, regression or classification models can be used to estimate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e53c75-e747-440d-ac9e-031b077b592a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
